---
title: Rimay Search
emoji: ðŸ§˜
colorFrom: red
colorTo: blue
sdk: docker
app_port: 7860
pinned: false
license: mit
---

Check out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference

rimay_search
==============================

search over Rimay teachings using natural language


TODO
------------

### Data preparation
  - [x] scrap website
  - [x] clean data (balanced articles in text length)

### Build search engine
  - [x] prototype
  - [x] applied to sample
  - [x] debug
    - [x] texts too long: need samples of max 1200 chars to follow openai
    This model's maximum context length is 4097 tokens, however you requested 9779 tokens (9523 in your prompt; 256 for the completion). Please reduce your prompt; or completion length
      - [NO] either reduce text size to 1200 chars as in langchain prompt (while I have a series of texts with 600 - 1200 tokens)
      - [x] or change architecture: QA to every match and combine, rather than QA on all matches in 1 prompt: map_reduce and map_ranks
    - [x] increase response size on demand
    - [X] track cost
    - [X] get url, chapter and everything from source returned into chromadb
    - [X] add title in response
  - [X] running on whole dataset
  - [ ] make sure to respond in the expected language (French)
  - [ ] [HIGH] break down texts longer than 1200 tokens so that we can get further details from chapter 1-2
  Ã  quoi servent les sciences contemplatives? -> This model's maximum context length is 4097 tokens, however you requested 8818 tokens (8562 in your prompt; 256 for the completion).
- [ ] FIX CLI CLICK not working with unit tests

### Build UI with streamlit
  - [X] build it with streamlit
    - [X] introduction to service
    - [X] input text to run query
    - [X] show response
      - [X] model answer
      - [X] links to sources
      - [X] metadata
      - [X] query debug info and cost
    - [ ] additional 
      - [X] format cost display
      - [X] add openapi token, hidden as password
      - [X] improve UI: admin vs normal yogi user (hide sidebar, json, token metadata details)
        - [X] Rewrite interface in French  
      - [ ] select language: French and English - https://blog.devgenius.io/how-to-build-a-multi-language-dashboard-with-streamlit-9bc087dd4243
      - [ ] response length

### Deploy app
- [X] manage db on distant server 
  - [X] explore hugging face spaces
  - [X] share data folder on git repo? 
  - [ ] or enable to run embedding on server? (probably not)
- [X] deploy on hugging face spaces

### Evaluate, share
- [ ] share and evaluate with karma ling
- [ ] write article 

Next steps
------------

- [ ] RÃ©duire le coÃ»t d'utilisation de l'application / la rendre plus rapide
   - [ ] utiliser un modÃ¨le plus petit (chatgptÂ ?)
     - [ ] changer de modÃ¨le
     - [ ] Fournir des exemples dÃ©diÃ©s pour le modÃ¨le de lecteur dans l'invite, adaptÃ©s Ã  la longueur du texte demandÃ© (en utilisant la FAQ sur la pleine conscience ouverte)
   - [ ] RÃ©duire la taille des documents
     - [ ] corrige le chapitre 1-2
     - [ ] Explorer la rÃ©duction des pagesÂ : dÃ©composer les pages en paragraphes + inclure le rÃ©sumÃ© de la page / les mÃ©tadonnÃ©esÂ ?

- [ ] Ã‰valuer les rÃ©ponses de l'application
   - [ ] recueillir les questions/rÃ©ponses
   - [ ] collecte de commentaires sur la configuration
     - [ ] bonne / mauvaise rÃ©ponse
     - [ ] meilleure rÃ©ponse manuelle - pour affiner plus tard
   - [ ] testez le modÃ¨le sur chacune de ces questions, Ã©valuez les rÃ©sultats, corrigez les rÃ©sultats

- [ ] AmÃ©liorer la qualitÃ© des rÃ©ponses
   - [ ] former le modÃ¨le de lecteur sur les contenus de pleine conscience ouverteÂ ?
   - [ ] testez diffÃ©rentes tailles d'articles (plus petits)

- [ ] Industrialiser l'application
   - [ ] DÃ©ploiement sur serveur dÃ©diÃ©
   - [ ] collecte des journaux de configuration (stacktrace)
   - [ ] Ã€ DÃ‰FINIR

- [ ] Ã‰tendre la base de donnÃ©es de connaissances
   - [ ] wiki bouddha -> voir avec TchamÃ© = 1 semaine
   - [ ] questions rÃ©ponses pendant les enseignements
   - [ ] sources externes
     - [ ] pages dÃ©diÃ©es au bouddhisme/mÃ©ditation sur wikipedia
     - [ ] recherche Google

Documentation
------------
- Retrieval question answering with sources based on [langchain dedicated tutorial](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html)
- Explanation of different kinds of chain (stuffing, map_reduce, refine, map_rerank), to deal with long queries to open_ai: in [langchain](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html), but also in this [excellent video](https://www.youtube.com/watch?v=), and this [cookbook](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook.ipynb). Here we are using map_reduce since the stuffing method, which tries to put all candidate documents in prompt to answer question, results in too long prompts for openai (over 4k) 
- modifying prompts, from [langchain](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html), to make sure it always responds in the requested language (when asking for long answers, it sometimes responds in English, since the prompt and examples given are in English)



Setup
------------
You will need to create a .env file and put your openai token

Warning: the tests are not mocked! So running some of those can be quickly costly - a qa search query on the state of the union costs 6cents, a qa search query on the open mindfulness contents can cost 40cents

## debug streamlit with vscode
add to launch.json
```
{
    "name": "debug streamlit",
    "type": "python",
    "request": "launch",
    "program": "/home/jeremie/anaconda3/envs/rimay_search/bin/streamlit",
    "args": [
        "run",
        "/home/jeremie/Documents/workspace/rimay_search/src/visualization/visualize.py"],
    "console": "integratedTerminal",
    "justMyCode": true
}
```
Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
